{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.4\n",
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from numpy import sqrt, sin, cos, pi, zeros\n",
    "from numpy.random import randn, rand, uniform, normal\n",
    "from scipy.linalg import hadamard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LSTM, Dropout, RepeatVector, TimeDistributed, Embedding, Reshape, Dot, Concatenate\n",
    "from tensorflow.keras.layers import GRU, SpatialDropout1D, Conv1D, GlobalMaxPooling1D,Multiply, Lambda, Softmax, Flatten, BatchNormalization, Bidirectional, dot, concatenate\n",
    "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Ball_1  Ball_2  Ball_3  Ball_4  Ball_5  Ball_6  Ball_Bonus\n",
      "Date                                                                  \n",
      "17/01/2023       1       5       9      12      18      36           6\n",
      "14/01/2023       4       5       9      18      19      35           2\n",
      "10/01/2023       1      26      28      31      34      36           3\n",
      "07/01/2023      10      14      17      20      31      36           5\n",
      "03/01/2023       2       6      29      30      31      32           2\n",
      "...            ...     ...     ...     ...     ...     ...         ...\n",
      "29/10/1968       3       9      26      32      34      36           1\n",
      "15/10/1968       1       6      23      29      33      34           7\n",
      "08/10/1968       7       9      10      17      23      34           4\n",
      "01/10/1968       1       7       8       9      11      30           4\n",
      "03/09/1968       3      14      18      22      25      33           2\n",
      "\n",
      "[2070 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "lotto = pd.read_csv('/workspaces/LottoIL/input/lottoIL_filt.csv', index_col = 'Date')\n",
    "print(lotto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lotto.values - 1\n",
    "train = data[:-50]\n",
    "test = data[-50:]\n",
    "\n",
    "w = 10\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(w, len(train)):\n",
    "    X_train.append(train[i - w: i, :])\n",
    "    y_train.append(train[i])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "inputs = data[data.shape[0] - test.shape[0] - w:]\n",
    "X_test = []\n",
    "for i in range(w, inputs.shape[0]):\n",
    "    X_test.append(inputs[i - w: i, :])\n",
    "X_test = np.array(X_test)\n",
    "y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 7)\n",
      "(2010, 10, 7)\n",
      "(2010, 7)\n",
      "(50, 10, 7)\n",
      "(50, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = 38\n",
    "embed_dim = 30\n",
    "dropout_rate = 0.5\n",
    "spatial_dropout_rate = 0.5\n",
    "steps_before = w\n",
    "steps_after = 7\n",
    "feature_count = embed_dim * 7\n",
    "hidden_neurons = [64, 32] \n",
    "bidirectional = True \n",
    "attention_style = 'Bahdanau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    inp0 = Input(shape = (w, X_train.shape[2]))\n",
    "    \n",
    "    # Embed numbers categories into a 30-dimension continuous-number vector for each ball\n",
    "    inp1 = Lambda(lambda x: x[:, :, 0])(inp0)\n",
    "    inp1 = Embedding(numbers, embed_dim)(inp1)\n",
    "    inp1 = SpatialDropout1D(spatial_dropout_rate)(inp1)\n",
    "    \n",
    "    inp2 = Lambda(lambda x: x[:, :, 1])(inp0)\n",
    "    inp2 = Embedding(numbers, embed_dim)(inp2)\n",
    "    inp2 = SpatialDropout1D(spatial_dropout_rate)(inp2)\n",
    "    \n",
    "    inp3 = Lambda(lambda x: x[:, :, 2])(inp0)\n",
    "    inp3 = Embedding(numbers, embed_dim)(inp3)\n",
    "    inp3 = SpatialDropout1D(spatial_dropout_rate)(inp3)\n",
    "    \n",
    "    inp4 = Lambda(lambda x: x[:, :, 3])(inp0)\n",
    "    inp4 = Embedding(numbers, embed_dim)(inp4)\n",
    "    inp4 = SpatialDropout1D(spatial_dropout_rate)(inp4)\n",
    "    \n",
    "    inp5 = Lambda(lambda x: x[:, :, 4])(inp0)\n",
    "    inp5 = Embedding(numbers, embed_dim)(inp5)\n",
    "    inp5 = SpatialDropout1D(spatial_dropout_rate)(inp5)    \n",
    "    \n",
    "    inp6 = Lambda(lambda x: x[:, :, 5])(inp0)\n",
    "    inp6 = Embedding(numbers, embed_dim)(inp6)\n",
    "    inp6 = SpatialDropout1D(spatial_dropout_rate)(inp6)\n",
    "    \n",
    "    inp7 = Lambda(lambda x: x[:, :, 6])(inp0)\n",
    "    inp7 = Embedding(numbers, embed_dim)(inp7)\n",
    "    inp7 = SpatialDropout1D(spatial_dropout_rate)(inp7)\n",
    "    \n",
    "    inp = Concatenate()([inp1, inp2, inp3, inp4, inp5, inp6, inp7])\n",
    "    \n",
    "    # Seq2Seq model with attention or bidirectional encoder\n",
    "    \n",
    "    num_layers = len(hidden_neurons)\n",
    "    \n",
    "    sh_list, h_list, c_list = [inp], [], []\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "    \n",
    "            sh, fh, fc, bh, bc = Bidirectional(LSTM(hidden_neurons[i],\n",
    "                                                    dropout = dropout_rate, \n",
    "                                                    return_state = True, \n",
    "                                                    return_sequences = True))(sh_list[-1])\n",
    "        \n",
    "            h = Concatenate()([fh, bh])\n",
    "            c = Concatenate()([fc, bc]) \n",
    "\n",
    "            sh_list.append(sh)\n",
    "            h_list.append(h)\n",
    "            c_list.append(c)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "\n",
    "            sh, h, c = LSTM(hidden_neurons[i], \n",
    "                            dropout = dropout_rate,\n",
    "                            return_state = True, \n",
    "                            return_sequences = True)(sh_list[-1])\n",
    "\n",
    "            sh_list.append(sh)\n",
    "            h_list.append(h)\n",
    "            c_list.append(c)\n",
    "    \n",
    "    decoder = RepeatVector(steps_after)(h_list[-1])\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        decoder_hidden_neurons = [hn * 2 for hn in hidden_neurons]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        decoder_hidden_neurons = hidden_neurons\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        decoder = LSTM(decoder_hidden_neurons[i],\n",
    "                       dropout = dropout_rate, \n",
    "                       return_sequences = True)(decoder, initial_state = [h_list[i], c_list[i]])\n",
    "       \n",
    "    if attention_style == 'Bahdanau':\n",
    "        \n",
    "        context = AdditiveAttention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
    "        \n",
    "        decoder = concatenate([context, decoder])\n",
    "        \n",
    "    elif attention_style == 'Luong':\n",
    "        \n",
    "        context = Attention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
    "        \n",
    "        decoder = concatenate([context, decoder])\n",
    "    \n",
    "    out = Dense(numbers, activation = 'softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs = inp0, outputs = out)\n",
    "    \n",
    "    sparse_top_k = tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 5, name = 'sparse_top_k')\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = [sparse_top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10, 7)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 10)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10, 30)       1140        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 30)       1140        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 10, 30)       1140        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10, 30)       1140        lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 10, 30)       1140        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 10, 30)       1140        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 10, 30)       1140        lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 10, 30)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 10, 30)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 10, 30)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 10, 30)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 10, 30)       0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 10, 30)       0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, 10, 30)       0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 210)      0           spatial_dropout1d[0][0]          \n",
      "                                                                 spatial_dropout1d_1[0][0]        \n",
      "                                                                 spatial_dropout1d_2[0][0]        \n",
      "                                                                 spatial_dropout1d_3[0][0]        \n",
      "                                                                 spatial_dropout1d_4[0][0]        \n",
      "                                                                 spatial_dropout1d_5[0][0]        \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 10, 128), (N 140800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 10, 64), (No 41216       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64)           0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 7, 64)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           bidirectional[0][2]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 7, 128)       98816       repeat_vector[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64)           0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 7, 64)        49408       lstm_2[0][0]                     \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "additive_attention (AdditiveAtt (None, 7, 64)        64          lstm_3[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 7, 128)       0           additive_attention[0][0]         \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 7, 38)        4902        concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 343,186\n",
      "Trainable params: 343,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes = True, show_layer_names = True, rankdir = 'TB', dpi = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingScheduler(callbacks.Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min = 0, verbose = 0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        backend.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = backend.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 - 4s - loss: 3.6333 - sparse_top_k: 0.1724 - val_loss: 3.6290 - val_sparse_top_k: 0.1886\n",
      "Epoch 2/200\n",
      "63/63 - 2s - loss: 3.6049 - sparse_top_k: 0.2043 - val_loss: 3.5681 - val_sparse_top_k: 0.1943\n",
      "Epoch 3/200\n",
      "63/63 - 2s - loss: 3.5224 - sparse_top_k: 0.2176 - val_loss: 3.4637 - val_sparse_top_k: 0.2143\n",
      "Epoch 4/200\n",
      "63/63 - 2s - loss: 3.3525 - sparse_top_k: 0.2753 - val_loss: 3.2568 - val_sparse_top_k: 0.3286\n",
      "Epoch 5/200\n",
      "63/63 - 2s - loss: 3.2049 - sparse_top_k: 0.3480 - val_loss: 3.1375 - val_sparse_top_k: 0.3686\n",
      "Epoch 6/200\n",
      "63/63 - 2s - loss: 3.1187 - sparse_top_k: 0.3881 - val_loss: 3.0596 - val_sparse_top_k: 0.4114\n",
      "Epoch 7/200\n",
      "63/63 - 2s - loss: 3.0592 - sparse_top_k: 0.4045 - val_loss: 3.0145 - val_sparse_top_k: 0.4229\n",
      "Epoch 8/200\n",
      "63/63 - 2s - loss: 3.0143 - sparse_top_k: 0.4178 - val_loss: 2.9747 - val_sparse_top_k: 0.4486\n",
      "Epoch 9/200\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "LR_MAX = 1e-4\n",
    "LR_MIN = 1e-5\n",
    "\n",
    "cas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n",
    "\n",
    "ckp = callbacks.ModelCheckpoint('best_model.hdf5', monitor = 'val_sparse_top_k', verbose = 0, \n",
    "                                save_best_only = True, save_weights_only = False, mode = 'max')\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test), \n",
    "                    callbacks = [ckp, cas], \n",
    "                    epochs = EPOCHS, \n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    verbose = 2)\n",
    "\n",
    "hist = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist['val_sparse_top_k'].max())\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.semilogy(hist['sparse_top_k'], '-r', label = 'Training')\n",
    "plt.semilogy(hist['val_sparse_top_k'], '-b', label = 'Validation')\n",
    "plt.ylabel('Sparse Top K Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epochs', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[0]):\n",
    "    print('Prediction:\\t', pred[i] + 1)\n",
    "    print('GoundTruth:\\t', y_test[i] + 1)\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "def beam_search_decoder(data, k, replace = True):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            best_k = np.argsort(row)[-k:]\n",
    "            for j in best_k:\n",
    "                candidate = [seq + [j], score + math.log(row[j])]\n",
    "                if replace:\n",
    "                    all_candidates.append(candidate)\n",
    "                elif (replace == False) and (len(set(candidate[0])) == len(candidate[0])):\n",
    "                    all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key = lambda tup:tup[1], reverse = True)\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_latest = X_test[-1][1:]\n",
    "X_latest = np.concatenate([X_latest, y_test[-1].reshape(1, 7)], axis = 0)\n",
    "X_latest = X_latest.reshape(1, X_latest.shape[0], X_latest.shape[1])\n",
    "print(X_latest + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_latest = model.predict(X_latest)\n",
    "pred_latest = np.squeeze(pred_latest)\n",
    "pred_latest_greedy = np.argmax(pred_latest, axis = 1)\n",
    "print(pred_latest_greedy + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "replace = True\n",
    "\n",
    "result = beam_search_decoder(pred_latest, beam_width, replace)\n",
    "print('Beam Width:\\t', beam_width)\n",
    "print('Replace:\\t', replace)\n",
    "print('-' * 85)\n",
    "for seq in result:\n",
    "    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "replace = False\n",
    "\n",
    "result = beam_search_decoder(pred_latest, beam_width, replace)\n",
    "print('Beam Width:\\t', beam_width)\n",
    "print('Replace:\\t', replace)\n",
    "print('-' * 85)\n",
    "for seq in result:\n",
    "    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
